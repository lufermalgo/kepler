# Kepler Framework - Gu√≠a Completa SDK y CLI

> **Versi√≥n del Framework:** 0.1.0  
> **√öltima actualizaci√≥n:** Diciembre 2024  
> **Estado:** Funcionalidades Core Validadas

## üìã Tabla de Contenidos

1. [Introducci√≥n y Filosof√≠a](#introducci√≥n-y-filosof√≠a)
2. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
3. [CLI - L√≠nea de Comandos](#cli---l√≠nea-de-comandos)
4. [SDK - Python API](#sdk---python-api)
5. [Casos de Uso Completos](#casos-de-uso-completos)
6. [Evoluci√≥n y Roadmap](#evoluci√≥n-y-roadmap)

---

## üéØ Introducci√≥n y Filosof√≠a

Kepler Framework ofrece **dos formas de interactuar** con la misma funcionalidad:

### **üñ•Ô∏è CLI (Command Line Interface)**
- Para **operaciones r√°pidas y scripts**
- Ideal para **DevOps y automatizaci√≥n**
- **Validaci√≥n de entorno** y diagn√≥sticos

### **üêç SDK (Software Development Kit)**
- Para **an√°lisis interactivo en Jupyter**
- Ideal para **cient√≠ficos de datos**
- **Integraci√≥n en notebooks** y scripts Python

> **Principio Clave:** Una sola l√≥gica, dos interfaces. Todo lo que hace el CLI tambi√©n lo puede hacer el SDK.

---

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n COMPLETA

### Paso 1: Instalaci√≥n del Framework

```bash
# Crear entorno virtual (RECOMENDADO)
python -m venv kepler-env
source kepler-env/bin/activate  # Linux/macOS
# kepler-env\Scripts\activate   # Windows

# Instalar desde GitHub (m√©todo actual)
git clone https://github.com/lufermalgo/kepler.git /tmp/kepler-install
cd /tmp/kepler-install
pip install .
cd ~ && rm -rf /tmp/kepler-install

# Verificar instalaci√≥n
kepler --version  # ‚úÖ Debe mostrar: 0.1.0
```

### Paso 2: Configuraci√≥n Inicial

```bash
# Crear estructura de configuraci√≥n
kepler config init

# Esto crea: ~/.kepler/config.yml
```

### Paso 3: Configurar Credenciales

Editar el archivo `~/.kepler/config.yml`:

```yaml
# ~/.kepler/config.yml
splunk:
  host: "http://localhost:8089"  # Tu servidor Splunk
  hec_host: "http://localhost:8088"  # HTTP Event Collector
  token: "tu-token-rest-api"     # Token para REST API
  hec_token: "tu-token-hec"      # Token para HEC
  verify_ssl: false              # Para desarrollo local

gcp:
  project_id: "tu-proyecto-gcp"
  region: "us-central1"
  credentials_path: "~/.gcp/service-account.json"
```

### Paso 4: Validar Configuraci√≥n

```bash
# Validaci√≥n completa en 5 pasos
kepler validate

# Salida esperada:
# ‚úÖ Step 1/5: Prerequisites validation
# ‚úÖ Step 2/5: GCP configuration  
# ‚úÖ Step 3/5: Project configuration
# ‚úÖ Step 4/5: Splunk connectivity
# ‚úÖ Step 5/5: Splunk indexes validation
```

---

## üñ•Ô∏è CLI - L√≠nea de Comandos

### **Comandos Principales (Validados y Funcionando)**

#### **1. `kepler validate` - Diagn√≥stico Completo**

```bash
# Validaci√≥n completa del entorno
kepler validate

# Validaci√≥n espec√≠fica de Splunk
kepler validate --splunk-only

# Validaci√≥n con output detallado
kepler validate --verbose
```

**Qu√© valida:**
- ‚úÖ Python 3.8+ instalado
- ‚úÖ Dependencias del framework
- ‚úÖ Configuraci√≥n GCP
- ‚úÖ Conectividad Splunk (REST API + HEC)
- ‚úÖ Existencia de √≠ndices `kepler_lab` y `kepler_metrics`
- ‚úÖ Auto-creaci√≥n de √≠ndices si no existen

#### **2. `kepler extract` - Extracci√≥n de Datos**

```bash
# Extracci√≥n b√°sica con SPL personalizado
kepler extract "search index=kepler_lab" --output data.csv

# Extracci√≥n con rango de tiempo
kepler extract "search index=kepler_lab sensor_type=temperature" \
    --earliest "-7d" --latest "now" --output temperature_data.csv

# Extracci√≥n de m√©tricas
kepler extract "| mstats avg(_value) WHERE index=kepler_metrics metric_name=* earliest=-30d span=1h by metric_name" \
    --output metrics_hourly.csv

# Extracci√≥n con l√≠mite de registros
kepler extract "search index=kepler_lab" --limit 1000 --output sample_data.csv
```

**Par√°metros disponibles:**
- `--output`: Archivo de salida (CSV)
- `--earliest`: Tiempo inicial (ej: `-7d`, `-24h`, `-1h`)
- `--latest`: Tiempo final (por defecto: `now`)
- `--limit`: M√°ximo n√∫mero de registros
- `--format`: Formato de salida (`csv`, `json`) - por defecto: `csv`

#### **3. `kepler config` - Gesti√≥n de Configuraci√≥n**

```bash
# Inicializar configuraci√≥n
kepler config init

# Mostrar configuraci√≥n actual (sin credenciales)
kepler config show

# Validar configuraci√≥n
kepler config validate

# Mostrar ubicaciones de archivos
kepler config paths
```

### **Comandos en Desarrollo (Pr√≥ximos Sprints)**

```bash
# üöß EN DESARROLLO - Sprint actual
kepler train data.csv --target temperature --algorithm random_forest
kepler train data.csv --target anomaly --algorithm xgboost --test-size 0.3

# üöß PLANEADO - Sprint 9-10  
kepler deploy model.pkl --name my-model --env production
kepler predict https://endpoint.run.app/predict '{"temperature": 25.5}'

# üöß PLANEADO - Sprint 11-12
kepler monitor --model my-model --dashboard splunk
kepler logs --model my-model --tail
```

---

## üêç SDK - Python API

### **Import y Configuraci√≥n**

```python
# Import s√∫per simple - sin configuraci√≥n manual
import kepler as kp

# El SDK usa autom√°ticamente la configuraci√≥n de ~/.kepler/config.yml
print(f"Kepler Framework version: {kp.__version__}")
```

### **Extracci√≥n de Datos con `kp.data.from_splunk()`**

#### **Extracci√≥n B√°sica de Eventos**

```python
# Eventos de sensores - √∫ltimos 7 d√≠as
eventos = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=temperature",
    earliest="-7d",
    latest="now"
)

print(f"Eventos extra√≠dos: {len(eventos)}")
print(f"Columnas: {list(eventos.columns)}")
print(eventos.head())
```

**Resultado validado:**
```
Eventos extra√≠dos: 867
Columnas: ['_time', 'sensor_id', 'sensor_type', 'area', 'value', 'unit']
```

#### **Extracci√≥n de M√©tricas**

```python
# M√©tricas - √∫ltimos 30 d√≠as  
metricas = kp.data.from_splunk(
    spl="| mstats latest(_value) as ultimo_valor WHERE index=kepler_metrics metric_name=* earliest=-30d by metric_name"
)

print(f"Tipos de m√©tricas: {len(metricas)}")
print("M√©tricas disponibles:")
for metrica in metricas['metric_name'].tolist():
    print(f"  - {metrica}")
```

**Resultado validado:**
```
Tipos de m√©tricas: 16
M√©tricas disponibles:
  - flow_rate.SENSOR_003
  - power_consumption.SENSOR_002
  - vibration.SENSOR_007
  - temperature.SENSOR_001
  [... y 12 m√°s]
```

#### **Control de Tiempo Avanzado**

```python
import pandas as pd

# Comparar diferentes rangos temporales
rangos = ["-1h", "-24h", "-7d", "-30d"]
resultados = {}

for rango in rangos:
    datos = kp.data.from_splunk(
        spl="search index=kepler_lab",
        earliest=rango,
        latest="now"
    )
    resultados[rango] = len(datos)
    
print("Datos por rango temporal:")
for rango, cantidad in resultados.items():
    print(f"  {rango}: {cantidad} registros")
```

**Resultado validado:**
```
Datos por rango temporal:
  -1h: 0 registros
  -24h: 90 registros  
  -7d: 2890 registros
  -30d: 2890 registros
```

### **SPL Personalizado Avanzado**

#### **An√°lisis de Series Temporales**

```python
# Series temporales de m√©tricas por hora
series_temporales = kp.data.from_splunk(
    spl="""
    | mstats avg(_value) as promedio 
    WHERE index=kepler_metrics metric_name=* earliest=-7d 
    span=1h by metric_name
    """
)

# Procesar para an√°lisis
series_temporales['timestamp'] = pd.to_datetime(series_temporales['_time'])
series_temporales['promedio'] = pd.to_numeric(series_temporales['promedio'])

# Estad√≠sticas por m√©trica
stats = series_temporales.groupby('metric_name')['promedio'].agg([
    'mean', 'std', 'min', 'max', 'count'
])

print("Estad√≠sticas por m√©trica:")
print(stats.head())
```

#### **An√°lisis de Eventos Complejos**

```python
# An√°lisis multi-sensor con filtros
analisis_sensores = kp.data.from_splunk(
    spl="""
    search index=kepler_lab 
    | eval hour=strftime(_time, "%H")
    | stats avg(value) as promedio, count as eventos by sensor_type, area, hour
    | where eventos > 5
    """,
    earliest="-7d"
)

print("An√°lisis por sensor, √°rea y hora:")
print(analisis_sensores.head(10))
```

### **Manejo de Errores Inteligente**

```python
# El SDK captura y muestra errores de Splunk claramente
try:
    datos = kp.data.from_splunk(
        spl="| mstats avg(_value) WHERE index=kepler_metrics"  # Query incompleta
    )
except Exception as e:
    print(f"Error capturado: {e}")
    # El framework muestra error espec√≠fico de Splunk + sugerencia
```

**Salida del manejo de errores:**
```
‚ùå Splunk Error: You must include at least one metric_name filter
üîç Query: | mstats avg(_value) WHERE index=kepler_metrics
üí° Tip: Check the SPL syntax according to Splunk documentation
```

---

## üìä Casos de Uso Completos

### **Caso 1: An√°lisis Exploratorio de Datos (Cient√≠fico de Datos)**

```python
import kepler as kp
import pandas as pd
import matplotlib.pyplot as plt

# 1. Explorar qu√© datos hay disponibles
print("=== EXPLORACI√ìN INICIAL ===")

# Ver m√©tricas disponibles  
metricas_disponibles = kp.data.from_splunk(
    spl="| mcatalog values(metric_name) WHERE index=kepler_metrics"
)
print(f"M√©tricas disponibles: {len(metricas_disponibles)}")

# Ver eventos recientes
eventos_recientes = kp.data.from_splunk(
    spl="search index=kepler_lab | head 100"
)
print(f"Tipos de sensores: {eventos_recientes['sensor_type'].unique()}")

# 2. An√°lisis temporal
print("\n=== AN√ÅLISIS TEMPORAL ===")

# Comparar vol√∫menes por per√≠odo
periodos = {
    "√öltima hora": "-1h",
    "√öltimas 24h": "-24h", 
    "√öltimos 7 d√≠as": "-7d"
}

for nombre, rango in periodos.items():
    datos = kp.data.from_splunk(
        spl="search index=kepler_lab",
        earliest=rango
    )
    print(f"{nombre}: {len(datos)} registros")

# 3. An√°lisis por tipo de sensor
print("\n=== AN√ÅLISIS POR SENSOR ===")

temperatura_data = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=temperature",
    earliest="-7d"
)

print(f"Datos de temperatura: {len(temperatura_data)} registros")
print(f"Rango temporal: {temperatura_data['_time'].min()} a {temperatura_data['_time'].max()}")
print(f"Estad√≠sticas b√°sicas:")
print(temperatura_data['value'].describe())
```

### **Caso 2: Pipeline de Datos Automatizado (DevOps/Engineer)**

```bash
#!/bin/bash
# extract_daily_data.sh - Pipeline automatizado

echo "=== PIPELINE DIARIO DE EXTRACCI√ìN ==="

# 1. Validar entorno
echo "Validando configuraci√≥n..."
kepler validate || exit 1

# 2. Extraer datos del d√≠a anterior
echo "Extrayendo eventos del d√≠a anterior..."
kepler extract "search index=kepler_lab earliest=-1d@d latest=@d" \
    --output "data/events_$(date +%Y%m%d).csv"

# 3. Extraer m√©tricas horarias
echo "Extrayendo m√©tricas horarias..."
kepler extract "| mstats avg(_value) WHERE index=kepler_metrics metric_name=* earliest=-1d@d latest=@d span=1h by metric_name" \
    --output "data/metrics_hourly_$(date +%Y%m%d).csv"

# 4. Generar reporte b√°sico
echo "Generando reporte..."
python generate_daily_report.py "data/events_$(date +%Y%m%d).csv"

echo "Pipeline completado exitosamente"
```

### **Caso 3: Monitoreo en Tiempo Real (Jupyter Notebook)**

```python
# Notebook: real_time_monitoring.ipynb

import kepler as kp
import time
from datetime import datetime

print("=== MONITOR EN TIEMPO REAL ===")

def monitor_loop():
    """Monitor continuo de m√©tricas cr√≠ticas"""
    
    while True:
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Checking metrics...")
        
        # Obtener m√©tricas de los √∫ltimos 15 minutos
        metricas_recientes = kp.data.from_splunk(
            spl="| mstats latest(_value) as valor WHERE index=kepler_metrics metric_name=power_consumption.* earliest=-15m by metric_name"
        )
        
        if len(metricas_recientes) > 0:
            # Detectar anomal√≠as simples
            for _, row in metricas_recientes.iterrows():
                metrica = row['metric_name']
                valor = float(row['valor'])
                
                # Umbral simple de ejemplo
                if valor > 1000:  # Consumo alto
                    print(f"‚ö†Ô∏è  ALERTA: {metrica} = {valor:.2f} (Alto consumo)")
                elif valor < 100:  # Consumo muy bajo  
                    print(f"üîç INFO: {metrica} = {valor:.2f} (Consumo bajo)")
                else:
                    print(f"‚úÖ OK: {metrica} = {valor:.2f}")
        else:
            print("‚ùå No se encontraron m√©tricas recientes")
            
        # Esperar 60 segundos antes del pr√≥ximo check
        time.sleep(60)

# Ejecutar monitor (detener con Ctrl+C)
try:
    monitor_loop()
except KeyboardInterrupt:
    print("\nMonitor detenido por el usuario")
```

---

## üîÑ Evoluci√≥n y Roadmap

### **‚úÖ SPRINT 1-8: FUNDACI√ìN (COMPLETADO)**

#### **CLI Implementado:**
- ‚úÖ `kepler validate` - Validaci√≥n completa en 5 pasos
- ‚úÖ `kepler extract` - Extracci√≥n con SPL personalizado
- ‚úÖ `kepler config` - Gesti√≥n de configuraci√≥n

#### **SDK Implementado:**
- ‚úÖ `import kepler as kp` - Import directo
- ‚úÖ `kp.data.from_splunk()` - Extracci√≥n flexible
- ‚úÖ Par√°metros `earliest`/`latest` para control temporal
- ‚úÖ Manejo inteligente de errores de Splunk

#### **Capacidades Validadas:**
- ‚úÖ **2,890 eventos** extra√≠dos exitosamente
- ‚úÖ **16 tipos de m√©tricas** funcionando
- ‚úÖ **Control temporal:** 32x diferencia entre 24h vs 7d
- ‚úÖ **Notebooks Jupyter** integraci√≥n completa

### **üöß SPRINT 9-10: ENTRENAMIENTO ML (EN DESARROLLO)**

#### **CLI Nuevo:**
```bash
# Comandos de ML que se van a implementar
kepler train data.csv --target temperature --algorithm random_forest
kepler train data.csv --target anomaly --algorithm xgboost --params config.json
kepler model list  # Listar modelos entrenados
kepler model info model_20241215_rf.pkl  # Info del modelo
```

#### **SDK Nuevo:**
```python
# API de ML que se va a implementar
model = kp.train.sklearn(
    data=eventos,
    target='temperature', 
    algorithm='RandomForest',
    test_size=0.3
)

# Predicciones locales
predictions = model.predict(test_data)
print(f"Accuracy: {model.metrics['accuracy']}")
```

### **üöß SPRINT 11-12: DEPLOYMENT GCP (PLANEADO)**

#### **CLI Deployment:**
```bash
# Deploy a Cloud Run
kepler deploy model.pkl --name temp-predictor --env production

# Gesti√≥n de deployments  
kepler deploy list
kepler deploy logs temp-predictor --tail
kepler deploy scale temp-predictor --instances 3
```

#### **SDK Deployment:**
```python
# Deploy program√°tico
endpoint = kp.deploy.cloud_run(
    model=trained_model,
    name="anomaly-detector",
    environment="production"
)

# Predicciones remotas
result = endpoint.predict({"sensor_data": [25.5, 2.1, 0.8]})
```

### **üöß SPRINT 13-16: PRODUCCI√ìN COMPLETA (FUTURO)**

#### **Funcionalidades Avanzadas:**
- üîÑ **Escritura autom√°tica** de predicciones a Splunk
- üìä **Dashboards autom√°ticos** en Splunk para monitoreo
- üîß **Pipeline completos** de CI/CD
- üì¶ **Distribuci√≥n PyPI** (`pip install kepler-framework`)

---

## üìö Recursos Adicionales

### **Notebooks de Ejemplo (Validados):**
- `test-lab/notebooks/metrics_analysis_clean.ipynb` - An√°lisis de m√©tricas paso a paso
- `test-lab/notebooks/events_analysis.ipynb` - An√°lisis de eventos completo

### **Archivos de Configuraci√≥n:**
- `~/.kepler/config.yml` - Configuraci√≥n global segura
- `proyecto/kepler.yml` - Configuraci√≥n espec√≠fica del proyecto

### **Documentaci√≥n T√©cnica:**
- `README.md` - Gu√≠a de inicio r√°pido
- `VALIDATION_STATUS.md` - Estado t√©cnico detallado
- Este archivo: `SDK_CLI_GUIDE.md` - Gu√≠a completa de uso

---

> **üí° Tip Final:** Comienza siempre con `kepler validate` para asegurar que todo est√© configurado correctamente. Luego usa `kepler extract` para explorar tus datos antes de entrenar modelos.

> **üéØ Estado Actual:** Las funciones de extracci√≥n y an√°lisis est√°n completamente validadas y listas para uso en producci√≥n. Las funciones de ML est√°n en desarrollo activo.