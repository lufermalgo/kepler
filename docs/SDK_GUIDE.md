# Kepler Framework - GuÃ­a SDK (Python API)

> **VersiÃ³n del Framework:** 0.1.0  
> **Ãšltima actualizaciÃ³n:** 6 de Septiembre de 2025  
> **Estado:** API Core Validada  
> **Audiencia:** CientÃ­ficos de Datos, Analistas, Desarrolladores Python

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n al SDK](#introducciÃ³n-al-sdk)
2. [InstalaciÃ³n y Setup](#instalaciÃ³n-y-setup)
3. [API de ExtracciÃ³n de Datos](#api-de-extracciÃ³n-de-datos)
4. [Jupyter Notebooks](#jupyter-notebooks)
5. [Casos de Uso Avanzados](#casos-de-uso-avanzados)
6. [EvoluciÃ³n del API](#evoluciÃ³n-del-api)

---

## ğŸ¯ IntroducciÃ³n al SDK

El **SDK de Kepler** estÃ¡ diseÃ±ado para:

### **ğŸ AnÃ¡lisis Interactivo**
- IntegraciÃ³n nativa con Jupyter notebooks
- API simple y familiar (similar a pandas/sklearn)
- Import directo sin configuraciÃ³n manual

### **ğŸ“Š Ciencia de Datos**
- ExtracciÃ³n flexible de datos desde Splunk
- Control temporal preciso para anÃ¡lisis histÃ³ricos
- IntegraciÃ³n con ecosystem Python (pandas, numpy, matplotlib)

### **ğŸ”¬ ExperimentaciÃ³n**
- Prototipado rÃ¡pido de modelos
- AnÃ¡lisis exploratorio de datos industriales
- ValidaciÃ³n de hipÃ³tesis con datos reales

---

## âš™ï¸ InstalaciÃ³n y Setup

### InstalaciÃ³n BÃ¡sica

```python
# Instalar Kepler (desde terminal)
# git clone https://github.com/lufermalgo/kepler.git /tmp/kepler-install  
# cd /tmp/kepler-install && pip install .
# rm -rf /tmp/kepler-install

# Import sÃºper simple - sin configuraciÃ³n manual
import kepler as kp

print(f"Kepler Framework version: {kp.__version__}")
```

### ConfiguraciÃ³n AutomÃ¡tica

El SDK usa automÃ¡ticamente la configuraciÃ³n de `~/.kepler/config.yml`:

```python
# No necesitas configurar nada manualmente
# El SDK carga automÃ¡ticamente:
# - Tokens de Splunk
# - ConfiguraciÃ³n GCP  
# - Preferencias de conexiÃ³n
```

---

## ğŸ“Š API de ExtracciÃ³n de Datos

### **FunciÃ³n Principal: `kp.data.from_splunk()`**

#### **ExtracciÃ³n BÃ¡sica de Eventos**

```python
import kepler as kp

# Eventos de sensores - Ãºltimos 7 dÃ­as
eventos = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=temperature",
    earliest="-7d",
    latest="now"
)

print(f"Eventos extraÃ­dos: {len(eventos)}")
print(f"Columnas: {list(eventos.columns)}")
print(eventos.head())
```

**Resultado validado:**
```
Eventos extraÃ­dos: 867
Columnas: ['_time', 'sensor_id', 'sensor_type', 'area', 'value', 'unit']
```

#### **ExtracciÃ³n de MÃ©tricas**

```python
# MÃ©tricas - Ãºltimos 30 dÃ­as  
metricas = kp.data.from_splunk(
    spl="| mstats latest(_value) as ultimo_valor WHERE index=kepler_metrics metric_name=* earliest=-30d by metric_name"
)

print(f"Tipos de mÃ©tricas: {len(metricas)}")
print("MÃ©tricas disponibles:")
for metrica in metricas['metric_name'].tolist():
    print(f"  - {metrica}")
```

**Resultado validado:**
```
Tipos de mÃ©tricas: 16
MÃ©tricas disponibles:
  - flow_rate.SENSOR_003
  - power_consumption.SENSOR_002
  - vibration.SENSOR_007
  - temperature.SENSOR_001
  [... y 12 mÃ¡s]
```

### **Control de Tiempo Avanzado**

```python
import pandas as pd

# Comparar diferentes rangos temporales
rangos = ["-1h", "-24h", "-7d", "-30d"]
resultados = {}

for rango in rangos:
    datos = kp.data.from_splunk(
        spl="search index=kepler_lab",
        earliest=rango,
        latest="now"
    )
    resultados[rango] = len(datos)
    
print("Datos por rango temporal:")
for rango, cantidad in resultados.items():
    print(f"  {rango}: {cantidad} registros")
```

**Resultado validado:**
```
Datos por rango temporal:
  -1h: 0 registros
  -24h: 90 registros  
  -7d: 2890 registros
  -30d: 2890 registros
```

### **SPL Personalizado Avanzado**

#### **AnÃ¡lisis de Series Temporales**

```python
# Series temporales de mÃ©tricas por hora
series_temporales = kp.data.from_splunk(
    spl="""
    | mstats avg(_value) as promedio 
    WHERE index=kepler_metrics metric_name=* earliest=-7d 
    span=1h by metric_name
    """
)

# Procesar para anÃ¡lisis
series_temporales['timestamp'] = pd.to_datetime(series_temporales['_time'])
series_temporales['promedio'] = pd.to_numeric(series_temporales['promedio'])

# EstadÃ­sticas por mÃ©trica
stats = series_temporales.groupby('metric_name')['promedio'].agg([
    'mean', 'std', 'min', 'max', 'count'
])

print("EstadÃ­sticas por mÃ©trica:")
print(stats.head())
```

#### **AnÃ¡lisis de Eventos Complejos**

```python
# AnÃ¡lisis multi-sensor con filtros
analisis_sensores = kp.data.from_splunk(
    spl="""
    search index=kepler_lab 
    | eval hour=strftime(_time, "%H")
    | stats avg(value) as promedio, count as eventos by sensor_type, area, hour
    | where eventos > 5
    """,
    earliest="-7d"
)

print("AnÃ¡lisis por sensor, Ã¡rea y hora:")
print(analisis_sensores.head(10))
```

### **Manejo de Errores Inteligente**

```python
# El SDK captura y muestra errores de Splunk claramente
try:
    datos = kp.data.from_splunk(
        spl="| mstats avg(_value) WHERE index=kepler_metrics"  # Query incompleta
    )
except Exception as e:
    print(f"Error capturado: {e}")
    # El framework muestra error especÃ­fico de Splunk + sugerencia
```

**Salida del manejo de errores:**
```
âŒ Splunk Error: You must include at least one metric_name filter
ğŸ” Query: | mstats avg(_value) WHERE index=kepler_metrics
ğŸ’¡ Tip: Check the SPL syntax according to Splunk documentation
```

---

## ğŸ““ Jupyter Notebooks

### **Setup en Notebook**

```python
# Celda 1: Import y configuraciÃ³n
import kepler as kp
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ConfiguraciÃ³n para notebooks (opcional)
import os
os.environ['KEPLER_NOTEBOOK_MODE'] = 'true'  # Salida limpia

print(f"âœ… Kepler {kp.__version__} cargado correctamente")
```

### **AnÃ¡lisis Exploratorio BÃ¡sico**

```python
# Celda 2: ExploraciÃ³n inicial
print("=== EXPLORACIÃ“N INICIAL ===")

# Ver quÃ© datos hay disponibles
eventos_recientes = kp.data.from_splunk(
    spl="search index=kepler_lab | head 100"
)

print(f"Tipos de sensores: {eventos_recientes['sensor_type'].unique()}")
print(f"Ãreas disponibles: {eventos_recientes['area'].unique()}")
print(f"Rango temporal: {eventos_recientes['_time'].min()} a {eventos_recientes['_time'].max()}")
```

### **VisualizaciÃ³n de Datos**

```python
# Celda 3: VisualizaciÃ³n
temperatura_data = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=temperature",
    earliest="-7d"
)

# Convertir timestamp
temperatura_data['timestamp'] = pd.to_datetime(temperatura_data['_time'])
temperatura_data['value'] = pd.to_numeric(temperatura_data['value'])

# GrÃ¡fico de series temporales
plt.figure(figsize=(12, 6))
for area in temperatura_data['area'].unique():
    data_area = temperatura_data[temperatura_data['area'] == area]
    plt.plot(data_area['timestamp'], data_area['value'], 
             label=f'Ãrea {area}', alpha=0.7)

plt.title('Temperatura por Ãrea - Ãšltimos 7 dÃ­as')
plt.xlabel('Tiempo')
plt.ylabel('Temperatura (Â°C)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

---

## ğŸ”¬ Casos de Uso Avanzados

### **Caso 1: AnÃ¡lisis de CorrelaciÃ³n**

```python
# Obtener datos de diferentes tipos de sensores
temperatura = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=temperature",
    earliest="-30d"
)

presion = kp.data.from_splunk(
    spl="search index=kepler_lab sensor_type=pressure", 
    earliest="-30d"
)

# AnÃ¡lisis de correlaciÃ³n por Ã¡rea
areas = temperatura['area'].unique()

for area in areas:
    temp_area = temperatura[temperatura['area'] == area]['value'].astype(float)
    pres_area = presion[presion['area'] == area]['value'].astype(float)
    
    if len(temp_area) > 10 and len(pres_area) > 10:
        correlation = np.corrcoef(temp_area[:min(len(temp_area), len(pres_area))], 
                                pres_area[:min(len(temp_area), len(pres_area))])[0,1]
        print(f"CorrelaciÃ³n Temperatura-PresiÃ³n en {area}: {correlation:.3f}")
```

### **Caso 2: DetecciÃ³n de AnomalÃ­as Simple**

```python
# Obtener mÃ©tricas de consumo de energÃ­a
energia = kp.data.from_splunk(
    spl="| mstats avg(_value) as consumo WHERE index=kepler_metrics metric_name=power_consumption.* earliest=-7d by metric_name",
)

# Detectar valores anÃ³malos (> 3 desviaciones estÃ¡ndar)
for _, row in energia.iterrows():
    sensor = row['metric_name']
    valor = float(row['consumo'])
    
    # Obtener historial para estadÃ­sticas
    historial = kp.data.from_splunk(
        spl=f"| mstats avg(_value) as valor WHERE index=kepler_metrics metric_name={sensor} earliest=-30d span=1h by _time"
    )
    
    if len(historial) > 10:
        valores = pd.to_numeric(historial['valor'])
        media = valores.mean()
        std = valores.std()
        
        if abs(valor - media) > 3 * std:
            print(f"âš ï¸ ANOMALÃA DETECTADA: {sensor}")
            print(f"   Valor actual: {valor:.2f}")
            print(f"   Media histÃ³rica: {media:.2f} Â± {std:.2f}")
```

### **Caso 3: AnÃ¡lisis de Tendencias**

```python
from scipy import stats

# Analizar tendencia de temperatura en el tiempo
temperatura_trend = kp.data.from_splunk(
    spl="""
    search index=kepler_lab sensor_type=temperature 
    | bucket _time span=1d 
    | stats avg(value) as temp_promedio by _time
    """,
    earliest="-30d"
)

# Convertir datos
temperatura_trend['timestamp'] = pd.to_datetime(temperatura_trend['_time'])
temperatura_trend['temp_promedio'] = pd.to_numeric(temperatura_trend['temp_promedio'])

# Calcular tendencia
x = np.arange(len(temperatura_trend))
y = temperatura_trend['temp_promedio'].values
slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)

print(f"Tendencia de temperatura:")
print(f"  Pendiente: {slope:.4f} Â°C/dÃ­a")
print(f"  CorrelaciÃ³n: {r_value:.3f}")
print(f"  P-value: {p_value:.3f}")

if p_value < 0.05:
    if slope > 0:
        print("  ğŸ“ˆ Tendencia al ALZA significativa")
    else:
        print("  ğŸ“‰ Tendencia a la BAJA significativa")
else:
    print("  â¡ï¸ Sin tendencia significativa")
```

---

## ğŸš§ API en Desarrollo

### **Sprint Actual - ML Training**

```python
# API de ML que se va a implementar
model = kp.train.sklearn(
    data=eventos,
    target='temperature', 
    algorithm='RandomForest',
    test_size=0.3
)

# Predicciones locales
predictions = model.predict(test_data)
print(f"Accuracy: {model.metrics['accuracy']}")
```

### **Sprint 11-12 - Deployment**

```python
# Deploy programÃ¡tico
endpoint = kp.deploy.cloud_run(
    model=trained_model,
    name="anomaly-detector",
    environment="production"
)

# Predicciones remotas
result = endpoint.predict({"sensor_data": [25.5, 2.1, 0.8]})
```

---

## ğŸ“š Notebooks de Ejemplo

### **Notebooks Validados Disponibles:**
- **`test-lab/notebooks/metrics_analysis_clean.ipynb`** - AnÃ¡lisis de mÃ©tricas paso a paso
- **`test-lab/notebooks/events_analysis.ipynb`** - AnÃ¡lisis de eventos completo

### **Estructura TÃ­pica de Notebook:**

```python
# Celda 1: Setup
import kepler as kp
import pandas as pd
import matplotlib.pyplot as plt

# Celda 2: ExploraciÃ³n
datos = kp.data.from_splunk(spl="search index=kepler_lab", earliest="-7d")
print(f"Datos extraÃ­dos: {len(datos)}")

# Celda 3: AnÃ¡lisis
# Tu anÃ¡lisis especÃ­fico aquÃ­...

# Celda 4: VisualizaciÃ³n  
# GrÃ¡ficos y plots...

# Celda 5: Conclusiones
# Insights y prÃ³ximos pasos...
```

---

## ğŸ¤– API de Entrenamiento Unificada

### **FunciÃ³n Principal: `kp.train_unified.train()`**

```python
import kepler as kp

# API unificada para cualquier framework AI
model = kp.train_unified.train(data, target="failure", algorithm="auto")

# Traditional ML
model = kp.train_unified.train(data, target="failure", algorithm="xgboost")
model = kp.train_unified.train(data, target="failure", algorithm="random_forest")

# Deep Learning  
model = kp.train_unified.train(data, target="failure", algorithm="pytorch", epochs=100)

# Generative AI
model = kp.train_unified.train(text_data, target="sentiment", algorithm="transformers", 
                              text_column="review_text")
```

### **AutoML Inteligente: `kp.automl.*`**

```python
# SelecciÃ³n automÃ¡tica de algoritmo
best_algo = kp.automl.select_algorithm(data, target="failure")
print(f"Mejor algoritmo: {best_algo}")

# Entrenamiento automÃ¡tico completo
model = kp.automl.auto_train(data, target="failure")

# AutoML con constraints industriales
industrial_result = kp.automl.industrial_automl(
    data, 
    target="equipment_failure",
    use_case="predictive_maintenance",
    optimization_budget="1h"
)
```

### **Sistema de Versionado MLOps: `kp.versioning.*`**

```python
# Crear versiÃ³n unificada (Git + DVC + MLflow)
version = kp.versioning.create_unified_version(
    "production-v1.0",
    data_paths=["data/sensors.csv"],
    experiment_name="predictive-maintenance"
)

# Reproducir cualquier versiÃ³n
result = kp.reproduce.from_version("production-v1.0")

# GestiÃ³n de releases
release = kp.versioning.create_release("stable-v1.0", status="production")
```

### **GestiÃ³n Ilimitada de LibrerÃ­as: `kp.libs.*`**

```python
# Instalar cualquier librerÃ­a Python
kp.libs.install("transformers>=4.30.0")
kp.libs.install("git+https://github.com/research/experimental-ai.git")

# Crear templates de AI
kp.libs.template("generative_ai")  # Instala transformers, langchain, etc.
kp.libs.template("deep_learning")  # Instala pytorch, tensorflow, etc.

# Validar entorno
status = kp.libs.validate()
```

---

## ğŸ”„ EvoluciÃ³n del API

### **âœ… Funcionalidades Actuales (0.1.0)**
- âœ… `kp.data.from_splunk()` - ExtracciÃ³n completa de datos
- âœ… `kp.train_unified.train()` - API unificada para cualquier framework AI
- âœ… `kp.automl.*` - Sistema AutoML completo (selecciÃ³n automÃ¡tica, optimizaciÃ³n)
- âœ… `kp.versioning.*` - Sistema MLOps completo (Git + DVC + MLflow)
- âœ… `kp.reproduce.from_version()` - Reproducibilidad completa
- âœ… `kp.libs.*` - Soporte ilimitado de librerÃ­as Python
- âœ… Control temporal con `earliest`/`latest`
- âœ… Manejo inteligente de errores con cÃ³digos estÃ¡ndar
- âœ… IntegraciÃ³n Jupyter optimizada

### **ğŸš§ PrÃ³ximas Versiones**
- ğŸ”„ `kp.deploy.*` - APIs de deployment automÃ¡tico
- ğŸ”„ `kp.monitor.*` - APIs de monitoreo hÃ­brido
- ğŸ”„ `kp.validate.*` - ValidaciÃ³n completa de ecosistemas
- ğŸ”„ `kp.docs.*` - GeneraciÃ³n automÃ¡tica de documentaciÃ³n

---

## ğŸ“‹ Recursos Adicionales

### **DocumentaciÃ³n Relacionada**
- **[CLI Guide](./CLI_GUIDE.md)** - Comandos de lÃ­nea para automatizaciÃ³n
- **[Estado de ValidaciÃ³n](./VALIDATION_STATUS.md)** - Funcionalidades probadas

### **Ejemplos PrÃ¡cticos**
- Notebooks en `test-lab/notebooks/`
- Scripts de ejemplo en `test-lab/scripts/`

### **Soporte y Comunidad**
- Issues en GitHub para reportar problemas
- DocumentaciÃ³n actualizada continuamente

---

> **ğŸ’¡ Tip para CientÃ­ficos:** El SDK estÃ¡ optimizado para anÃ¡lisis exploratorio. Usa `earliest` y `latest` para controlar exactamente quÃ© datos necesitas para tu anÃ¡lisis.

> **ğŸ¯ Estado Actual:** La API de extracciÃ³n estÃ¡ completamente validada con datos reales (2,890 eventos + 16 mÃ©tricas) y lista para uso en anÃ¡lisis de producciÃ³n.